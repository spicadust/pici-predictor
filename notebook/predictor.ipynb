{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal: train 10 machine learning predictors: one predictor for each function from the ten protein function categories (\"DNA, RNA and nucleotide metabolism\", \"tail\", \"head and packaging\", \"other\", \"lysis\", \"connector\", \"transcription regulation\", \"moron, auxiliary metabolic gene and host takeover\", \"unknown function\", \"integration and excision\") \n",
    "\n",
    "predictor input: protein features; output: labels (0/1) representing whether the protein serves the specific function\n",
    "\n",
    "dataset: 360,413 seqs in total - 60% for training, 20% for validation, 20% for testing\n",
    "-use clustering results to avoid spliting protein seqs in the same cluster (maybe use GroupShuffleSplit from sklearn)\n",
    "\n",
    "features: 1711-dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset:  \n",
    "2,318,538 seqs in the original dataset  \n",
    "927,040 seqs after dropping pcat \"unknown_no_hit\"  \n",
    "360,413 unique seqs after dropping duplicated seqs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset the negatives;  \n",
    "for each function, cluster on the positive dataset  \n",
    "try threshold from 0.01 to 1  \n",
    "change the dataset splitting strategy  \n",
    "model param : is_imbalanced  \n",
    "add learning curve (could be find from XGBoost)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict, Any\n",
    "import joblib\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(function_name: str):\n",
    "    ids = pd.read_csv(f\"../dataset/pcat/{function_name}.csv\")\n",
    "    features = pd.read_parquet(\"../dataset/protein_features_unique.pa\")\n",
    "    features[\"label\"] = features[\"id\"].isin(ids[\"name\"]).astype(int)\n",
    "    features = features.drop(columns=[\"md5\"])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: list,\n",
    "    cluster_mapping: Dict[str, str],\n",
    "    label_col: str = \"label\",\n",
    "    id_col: str = \"id\",\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare data for training by separating features and labels.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing features and labels\n",
    "        feature_cols: List of feature column names\n",
    "        cluster_mapping: Dictionary mapping sequence IDs to cluster IDs\n",
    "        label_col: Name of the label column\n",
    "        id_col: Name of the ID column\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix\n",
    "        y: Label array\n",
    "        ids: Array of sequence IDs\n",
    "        groups: Array of cluster IDs for each sequence\n",
    "    \"\"\"\n",
    "    X = df[feature_cols].values\n",
    "    y = df[label_col].values\n",
    "    ids = df[id_col].values\n",
    "\n",
    "    # Get cluster IDs for each sequence\n",
    "    groups = np.array([cluster_mapping.get(str(id_), \"unknown\") for id_ in ids])\n",
    "\n",
    "    return X, y, ids, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    ids: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.2,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Split data into train, validation, and test sets while keeping related sequences together.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Label array\n",
    "        ids: Array of sequence IDs\n",
    "        groups: Array of cluster IDs for each sequence\n",
    "        test_size: Proportion of data to use for testing\n",
    "        val_size: Proportion of data to use for validation\n",
    "    \"\"\"\n",
    "    # First split: separate test set\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "    train_val_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "\n",
    "    # Second split: separate validation set from training set\n",
    "    val_size_adjusted = val_size / (\n",
    "        1 - test_size\n",
    "    )  # Adjust val_size to account for test set\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=val_size_adjusted, random_state=42)\n",
    "    train_idx, val_idx = next(\n",
    "        gss.split(X_train_val, y_train_val, groups=groups_train_val)\n",
    "    )\n",
    "\n",
    "    X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "    y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# first split: test 20%\n",
    "# train:val = 8:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    X_train: np.ndarray, y_train: np.ndarray, model_params: Dict[str, Any] = None\n",
    ") -> Tuple[Any, StandardScaler]:\n",
    "    \"\"\"Train an XGBoost classifier with optional hyperparameters.\"\"\"\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Set default parameters if none provided\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            \"n_estimators\": 200,\n",
    "            \"max_depth\": 6,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"min_child_weight\": 1,\n",
    "            \"scale_pos_weight\": 1,  # Will be adjusted based on class imbalance\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "    # Calculate scale_pos_weight based on class imbalance\n",
    "    n_neg = np.sum(y_train == 0)\n",
    "    n_pos = np.sum(y_train == 1)\n",
    "    model_params[\"scale_pos_weight\"] = n_neg / n_pos\n",
    "\n",
    "    # Train model\n",
    "    model = XGBClassifier(**model_params)\n",
    "    model.fit(\n",
    "        # X_train_scaled, y_train, eval_set=[(X_train_scaled, y_train)], verbose=False\n",
    "    )  # x_val_scaled, y_val\n",
    "\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: Any,\n",
    "    scaler: StandardScaler,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    set_name: str = \"\",\n",
    "    threshold: float = 0.5,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate model performance on a dataset.\"\"\"\n",
    "\n",
    "    # no need for scaling!\n",
    "\n",
    "    X_scaled = scaler.transform(X)\n",
    "    y_pred_proba = model.predict_proba(X_scaled)[:, 1]\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    # Calculate MCC\n",
    "    def matthews(y_true, y_pred):\n",
    "        from math import sqrt\n",
    "\n",
    "        \"\"\"\n",
    "            P  = Total number of positives\n",
    "            N  = Total number of negatives\n",
    "            Tp = number of true positives\n",
    "            Fp = number of false positives\n",
    "        \"\"\"\n",
    "        if type(y_true) == pd.Series:\n",
    "            y_true = y_true.values\n",
    "\n",
    "        P = len([x for x in y_true if x == 1])\n",
    "        N = len([x for x in y_true if x == 0])\n",
    "\n",
    "        Tp, Fp = 0, 0\n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i] == 1 and y_pred[i] == 1:\n",
    "                Tp += 1\n",
    "            elif y_true[i] == 0 and y_pred[i] == 1:\n",
    "                Fp += 1\n",
    "\n",
    "        Tn = N - Fp\n",
    "        Fn = P - Tp\n",
    "\n",
    "        try:\n",
    "            mcc = (Tp * Tn - Fp * Fn) / sqrt(\n",
    "                (Tn + Fn) * (Tn + Fp) * (Tp + Fn) * (Tp + Fp)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            mcc = 0\n",
    "        return (mcc, f\"P: {P:_} Tp: {Tp:_} Fp: {Fp:_} N: {N:_} Tn: {Tn:_} Fn: {Fn:_}\")\n",
    "\n",
    "    # Get MCC and confusion matrix values\n",
    "    mcc, confusion_str = matthews(y, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        f\"{set_name}_accuracy\": accuracy_score(y, y_pred),\n",
    "        f\"{set_name}_precision\": precision_score(y, y_pred),\n",
    "        f\"{set_name}_recall\": recall_score(y, y_pred),\n",
    "        f\"{set_name}_f1\": f1_score(y, y_pred),\n",
    "        f\"{set_name}_roc_auc\": roc_auc_score(y, y_pred_proba),\n",
    "        f\"{set_name}_mcc\": mcc,\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\nMetrics for {set_name} (threshold={threshold}):\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Print confusion matrix values\n",
    "    print(f\"\\nConfusion Matrix Values:\")\n",
    "    print(confusion_str)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_feature_importance(model: Any, feature_cols: list, top_n: int = 20):\n",
    "#     \"\"\"Plot feature importance from XGBoost model.\"\"\"\n",
    "#     importance_scores = model.feature_importances_\n",
    "#     indices = np.argsort(importance_scores)[::-1]\n",
    "\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.title(\"Feature Importances\")\n",
    "#     plt.bar(range(top_n), importance_scores[indices[:top_n]])\n",
    "#     plt.xticks(range(top_n), [feature_cols[i] for i in indices[:top_n]], rotation=90)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Print top N most important features\n",
    "#     print(f\"\\nTop {top_n} most important features:\")\n",
    "#     for i in range(top_n):\n",
    "#         print(f\"{feature_cols[indices[i]]}: {importance_scores[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(\n",
    "    model: Any, scaler: StandardScaler, feature_cols: list, function_name: str\n",
    "):\n",
    "    \"\"\"Save the trained model and scaler.\"\"\"\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "    # Create a dictionary containing all necessary components\n",
    "    model_data = {\"model\": model, \"scaler\": scaler, \"feature_cols\": feature_cols}\n",
    "\n",
    "    # Save the model data\n",
    "    joblib.dump(model_data, f\"../models/{function_name}_predictor.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function_predictor(function_name: str, model_params: Dict[str, Any] = None):\n",
    "    \"\"\"Main training pipeline for a specific protein function.\"\"\"\n",
    "    # Load cluster mapping\n",
    "    with open(\"../dataset/protein_cluster_mapping.json\", \"r\") as f:\n",
    "        cluster_mapping = json.load(f)\n",
    "\n",
    "    # get df using function name\n",
    "    df = get_df(function_name)\n",
    "\n",
    "    # Get feature columns (excluding 'id' and 'label')\n",
    "    feature_cols = [col for col in df.columns if col not in [\"id\", \"label\"]]\n",
    "\n",
    "    # Prepare data\n",
    "    X, y, ids, groups = prepare_data(df, feature_cols, cluster_mapping)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, ids, groups)\n",
    "\n",
    "    # Train model\n",
    "    model, scaler = train_model(X_train, y_train, model_params)\n",
    "\n",
    "    # Print feature importance (without plot)\n",
    "    importance_scores = model.feature_importances_\n",
    "    indices = np.argsort(importance_scores)[::-1]\n",
    "    print(\"\\nTop 20 most important features:\")\n",
    "    for i in range(20):\n",
    "        print(f\"{feature_cols[indices[i]]}: {importance_scores[indices[i]]:.4f}\")\n",
    "\n",
    "    # Evaluate model on all sets\n",
    "    print(\"\\n=== Training Set ===\")\n",
    "    train_metrics = evaluate_model(model, scaler, X_train, y_train, \"train\")\n",
    "\n",
    "    print(\"\\n=== Validation Set ===\")\n",
    "    val_metrics = evaluate_model(model, scaler, X_val, y_val, \"val\")\n",
    "\n",
    "    print(\"\\n=== Test Set ===\")\n",
    "    # Calculate metrics for all thresholds\n",
    "    thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]  # try 0.01 - 1\n",
    "    for threshold in thresholds:\n",
    "        test_metrics = evaluate_model(model, scaler, X_test, y_test, \"test\", threshold)\n",
    "\n",
    "    # Save model\n",
    "    save_model(model, scaler, feature_cols, function_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 most important features:\n",
      "BP:prop_res_Basic: 0.0218\n",
      "BP:isoelectric_point: 0.0094\n",
      "RED_TRIPEP:PSC: 0.0090\n",
      "DIPEP:SD: 0.0081\n",
      "BP:length: 0.0080\n",
      "BP:percent:R: 0.0072\n",
      "BP:percent:C: 0.0071\n",
      "RED_TRIPEP:KKH: 0.0070\n",
      "BP:molecular_weight: 0.0057\n",
      "RED_TRIPEP:AFH: 0.0051\n",
      "RED_TRIPEP:FCC: 0.0047\n",
      "RED_TRIPEP:FKP: 0.0043\n",
      "RED_TRIPEP:HCG: 0.0043\n",
      "RED_DIPEP:KK: 0.0039\n",
      "RED_TRIPEP:SCK: 0.0039\n",
      "DIPEP:DH: 0.0038\n",
      "BP:molar_extinction_coefficient_cysteines: 0.0038\n",
      "RED_TRIPEP:HSL: 0.0036\n",
      "RED_TRIPEP:CCL: 0.0034\n",
      "DIPEP:TC: 0.0033\n",
      "\n",
      "=== Training Set ===\n",
      "\n",
      "Metrics for train (threshold=0.5):\n",
      "train_accuracy: 0.9995\n",
      "train_precision: 0.9789\n",
      "train_recall: 1.0000\n",
      "train_f1: 0.9893\n",
      "train_roc_auc: 1.0000\n",
      "train_mcc: 0.9892\n",
      "\n",
      "Confusion Matrix Values:\n",
      "P: 4_273 Tp: 4_273 Fp: 92 N: 178_385 Tn: 178_293 Fn: 0\n",
      "\n",
      "=== Validation Set ===\n",
      "\n",
      "Metrics for val (threshold=0.5):\n",
      "val_accuracy: 0.9889\n",
      "val_precision: 0.7159\n",
      "val_recall: 0.7507\n",
      "val_f1: 0.7329\n",
      "val_roc_auc: 0.9781\n",
      "val_mcc: 0.7274\n",
      "\n",
      "Confusion Matrix Values:\n",
      "P: 2_286 Tp: 1_716 Fp: 681 N: 110_012 Tn: 109_331 Fn: 570\n",
      "\n",
      "=== Test Set ===\n",
      "\n",
      "Metrics for test (threshold=0.3):\n",
      "test_accuracy: 0.9842\n",
      "test_precision: 0.3861\n",
      "test_recall: 0.3037\n",
      "test_f1: 0.3399\n",
      "test_roc_auc: 0.8557\n",
      "test_mcc: 0.3345\n",
      "\n",
      "Confusion Matrix Values:\n",
      "P: 876 Tp: 266 Fp: 423 N: 64_581 Tn: 64_158 Fn: 610\n",
      "\n",
      "Metrics for test (threshold=0.4):\n",
      "test_accuracy: 0.9853\n",
      "test_precision: 0.4095\n",
      "test_recall: 0.2272\n",
      "test_f1: 0.2922\n",
      "test_roc_auc: 0.8557\n",
      "test_mcc: 0.2981\n",
      "\n",
      "Confusion Matrix Values:\n",
      "P: 876 Tp: 199 Fp: 287 N: 64_581 Tn: 64_294 Fn: 677\n",
      "\n",
      "Metrics for test (threshold=0.5):\n",
      "test_accuracy: 0.9860\n",
      "test_precision: 0.4466\n",
      "test_recall: 0.1815\n",
      "test_f1: 0.2581\n",
      "test_roc_auc: 0.8557\n",
      "test_mcc: 0.2788\n",
      "\n",
      "Confusion Matrix Values:\n",
      "P: 876 Tp: 159 Fp: 197 N: 64_581 Tn: 64_384 Fn: 717\n",
      "\n",
      "Metrics for test (threshold=0.6):\n",
      "test_accuracy: 0.9865\n",
      "test_precision: 0.4874\n",
      "test_recall: 0.1541\n",
      "test_f1: 0.2342\n",
      "test_roc_auc: 0.8557\n",
      "test_mcc: 0.2689\n",
      "\n",
      "Confusion Matrix Values:\n",
      "P: 876 Tp: 135 Fp: 142 N: 64_581 Tn: 64_439 Fn: 741\n",
      "\n",
      "Metrics for test (threshold=0.7):\n",
      "test_accuracy: 0.9864\n",
      "test_precision: 0.4712\n",
      "test_recall: 0.1119\n",
      "test_f1: 0.1808\n",
      "test_roc_auc: 0.8557\n",
      "test_mcc: 0.2249\n",
      "\n",
      "Confusion Matrix Values:\n",
      "P: 876 Tp: 98 Fp: 110 N: 64_581 Tn: 64_471 Fn: 778\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    \"n_estimators\": 300,  # 10k maybe\n",
    "    \"max_depth\": 8,  # Increase from 6\n",
    "    \"learning_rate\": 0.05,  # Decrease from 0.1\n",
    "    \"subsample\": 0.7,  # Decrease from 0.8\n",
    "    \"colsample_bytree\": 0.7,  # Decrease from 0.8\n",
    "    \"min_child_weight\": 2,  # Increase from 1\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "train_function_predictor(\"integration_and_excision\", model_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
