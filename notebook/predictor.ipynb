{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal: train 10 machine learning predictors: one predictor for each function from the ten protein function categories (\"DNA, RNA and nucleotide metabolism\", \"tail\", \"head and packaging\", \"other\", \"lysis\", \"connector\", \"transcription regulation\", \"moron, auxiliary metabolic gene and host takeover\", \"unknown function\", \"integration and excision\") \n",
    "\n",
    "predictor input: protein features; output: labels (0/1) representing whether the protein serves the specific function\n",
    "\n",
    "dataset: 360,413 seqs in total - 60% for training, 20% for validation, 20% for testing \n",
    "** subset the negatives (samples whose label is 0) and make it about 5-10 times of the positives\n",
    "** change: split 20% for testing first, then train:val = 8:2\n",
    "\n",
    "-use clustering results to avoid spliting protein seqs in the same cluster (maybe use GroupShuffleSplit from sklearn)\n",
    "** change: for each predictor, cluster on the positive dataset (those with label \"1\") alone, and do nothing to negative dataset. use the clustering result while splitting dataset to ensure seqs in the same cluster in positive dataset is not splitted into different sets of train/val/test. I already have protein distances generated from diamond blastp in unique_diamond_results.daa, and would like to get the clustering result by select a cutoff of 100bitscore and use that to read into a graph with networkx and then extract subgraphs which then will be the clusters.\n",
    "\n",
    "** change: add learning curve for training XGBoost\n",
    "\n",
    "** change: try the 100 versions of threshold from 0.01 - 1\n",
    "\n",
    "the results are printed as text.  \n",
    "** change: not to print them out but to save it to results/predictor/{function_name}.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset:  \n",
    "2,318,538 seqs in the original dataset  \n",
    "927,040 seqs after dropping pcat \"unknown_no_hit\"  \n",
    "360,413 unique seqs after dropping duplicated seqs  \n",
    "\n",
    "features: 1711-dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset the negatives;  \n",
    "- for each function, cluster on the positive dataset  \n",
    "- try threshold from 0.01 to 1  \n",
    "- [o] change the dataset splitting strategy  \n",
    "- [o] model param : is_imbalanced  \n",
    "- add learning curve (could be find from XGBoost)  \n",
    "- n_estimators: increased to 10k; may need GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict, Any\n",
    "import joblib\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(function_name: str):\n",
    "    ids = pd.read_csv(f\"../dataset/pcat/{function_name}.csv\")\n",
    "    features = pd.read_parquet(\"../dataset/protein_features_unique_no_dipep_tripep.pa\")\n",
    "    features[\"label\"] = features[\"id\"].isin(ids[\"name\"]).astype(int)\n",
    "    # features = features.drop(columns=[\"md5\"])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_negatives(\n",
    "    df: pd.DataFrame, label_col: str = \"label\", ratio: int = 5, random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subset the negatives to be `ratio` times the number of positives.\n",
    "    Args:\n",
    "        df: DataFrame with a label column\n",
    "        label_col: Name of the label column\n",
    "        ratio: Negative:positive ratio\n",
    "        random_state: For reproducibility\n",
    "    Returns:\n",
    "        Subsetted DataFrame\n",
    "    \"\"\"\n",
    "    pos_df = df[df[label_col] == 1]\n",
    "    neg_df = df[df[label_col] == 0]\n",
    "    n_pos = len(pos_df)\n",
    "    n_neg = min(len(neg_df), n_pos * ratio)\n",
    "    neg_df = neg_df.sample(n=n_neg, random_state=random_state)\n",
    "    return (\n",
    "        pd.concat([pos_df, neg_df])\n",
    "        .sample(frac=1, random_state=random_state)\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_positives(\n",
    "    df, bitscore_file, bitscore_cutoff=100, id_col=\"id\", label_col=\"label\"\n",
    "):\n",
    "    pos_ids = set(df[df[label_col] == 1][id_col])\n",
    "\n",
    "    # Load diamond results (parquet)\n",
    "    diamond = pd.read_parquet(bitscore_file)\n",
    "    # If needed, rename columns here:\n",
    "    # diamond = diamond.rename(columns={\"col1\": \"qseqid\", \"col2\": \"sseqid\", \"col3\": \"bitscore\"})\n",
    "\n",
    "    # Filter for bitscore cutoff and only positive IDs\n",
    "    diamond = diamond[\n",
    "        (diamond[\"bitscore\"] >= bitscore_cutoff)\n",
    "        & (diamond[\"qseqid\"].isin(pos_ids))\n",
    "        & (diamond[\"sseqid\"].isin(pos_ids))\n",
    "    ]\n",
    "\n",
    "    # Build graph\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(diamond[[\"qseqid\", \"sseqid\"]].itertuples(index=False, name=None))\n",
    "\n",
    "    # Assign cluster IDs\n",
    "    cluster_mapping = {}\n",
    "    for i, component in enumerate(nx.connected_components(G)):\n",
    "        for node in component:\n",
    "            cluster_mapping[node] = f\"cluster_{i}\"\n",
    "\n",
    "    # Assign unconnected positives to their own cluster\n",
    "    for pid in pos_ids:\n",
    "        if pid not in cluster_mapping:\n",
    "            cluster_mapping[pid] = f\"cluster_single_{pid}\"\n",
    "\n",
    "    return cluster_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: list,\n",
    "    cluster_mapping: Dict[str, str],\n",
    "    label_col: str = \"label\",\n",
    "    id_col: str = \"id\",\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare data for training by separating features and labels.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing features and labels\n",
    "        feature_cols: List of feature column names\n",
    "        cluster_mapping: Dictionary mapping sequence IDs to cluster IDs\n",
    "        label_col: Name of the label column\n",
    "        id_col: Name of the ID column\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix\n",
    "        y: Label array\n",
    "        ids: Array of sequence IDs\n",
    "        groups: Array of cluster IDs for each sequence\n",
    "    \"\"\"\n",
    "    X = df[feature_cols].values\n",
    "    y = df[label_col].values\n",
    "    ids = df[id_col].values\n",
    "\n",
    "    # Get cluster IDs for each sequence\n",
    "    groups = np.array([cluster_mapping.get(str(id_), \"unknown\") for id_ in ids])\n",
    "\n",
    "    return X, y, ids, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    ids: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.2,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Split data into train, validation, and test sets while keeping related sequences together.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Label array\n",
    "        ids: Array of sequence IDs\n",
    "        groups: Array of cluster IDs for each sequence\n",
    "        test_size: Proportion of data to use for testing\n",
    "        val_size: Proportion of data to use for validation\n",
    "    \"\"\"\n",
    "    # First split: separate test set\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "    train_val_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "\n",
    "    # Second split: separate validation set from training set\n",
    "    val_size_adjusted = val_size / (\n",
    "        1 - test_size\n",
    "    )  # Adjust val_size to account for test set\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=val_size_adjusted, random_state=42)\n",
    "    train_idx, val_idx = next(\n",
    "        gss.split(X_train_val, y_train_val, groups=groups_train_val)\n",
    "    )\n",
    "\n",
    "    X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "    y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# first split: test 20%\n",
    "# train:val = 8:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    model_params: Dict[str, Any] = None,\n",
    ") -> Tuple[Any, StandardScaler]:\n",
    "    \"\"\"Train an XGBoost classifier with optional hyperparameters.\"\"\"\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Set default parameters if none provided\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            \"n_estimators\": 200,\n",
    "            \"max_depth\": 6,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"min_child_weight\": 1,\n",
    "            \"scale_pos_weight\": 1,\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "    # Calculate scale_pos_weight based on class imbalance\n",
    "    n_neg = np.sum(y_train == 0)\n",
    "    n_pos = np.sum(y_train == 1)\n",
    "    model_params[\"scale_pos_weight\"] = n_neg / n_pos\n",
    "\n",
    "    # Train model\n",
    "    model = XGBClassifier(eval_metric=[\"logloss\", \"auc\"], **model_params)\n",
    "    model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        eval_set=[(X_train_scaled, y_train), (X_val_scaled, y_val)],\n",
    "        verbose=False,\n",
    "        # early_stopping_rounds=50,\n",
    "    )\n",
    "    evals_result = model.evals_result()\n",
    "    return model, scaler, evals_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: Any,\n",
    "    scaler: StandardScaler,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    set_name: str = \"\",\n",
    "    threshold: float = 0.5,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate model performance on a dataset.\"\"\"\n",
    "\n",
    "    # no need for scaling!\n",
    "\n",
    "    X_scaled = scaler.transform(X)\n",
    "    y_pred_proba = model.predict_proba(X_scaled)[:, 1]\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    # Calculate MCC\n",
    "    def matthews(y_true, y_pred):\n",
    "        from math import sqrt\n",
    "\n",
    "        \"\"\"\n",
    "            P  = Total number of positives\n",
    "            N  = Total number of negatives\n",
    "            Tp = number of true positives\n",
    "            Fp = number of false positives\n",
    "        \"\"\"\n",
    "        if type(y_true) == pd.Series:\n",
    "            y_true = y_true.values\n",
    "\n",
    "        P = len([x for x in y_true if x == 1])\n",
    "        N = len([x for x in y_true if x == 0])\n",
    "\n",
    "        Tp, Fp = 0, 0\n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i] == 1 and y_pred[i] == 1:\n",
    "                Tp += 1\n",
    "            elif y_true[i] == 0 and y_pred[i] == 1:\n",
    "                Fp += 1\n",
    "\n",
    "        Tn = N - Fp\n",
    "        Fn = P - Tp\n",
    "\n",
    "        try:\n",
    "            mcc = (Tp * Tn - Fp * Fn) / sqrt(\n",
    "                (Tn + Fn) * (Tn + Fp) * (Tp + Fn) * (Tp + Fp)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            mcc = 0\n",
    "        return (mcc, f\"P: {P:_} Tp: {Tp:_} Fp: {Fp:_} N: {N:_} Tn: {Tn:_} Fn: {Fn:_}\")\n",
    "\n",
    "    # Get MCC and confusion matrix values\n",
    "    mcc, confusion_str = matthews(y, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        f\"{set_name}_accuracy\": accuracy_score(y, y_pred),\n",
    "        f\"{set_name}_precision\": precision_score(y, y_pred),\n",
    "        f\"{set_name}_recall\": recall_score(y, y_pred),\n",
    "        f\"{set_name}_f1\": f1_score(y, y_pred),\n",
    "        f\"{set_name}_roc_auc\": roc_auc_score(y, y_pred_proba),\n",
    "        f\"{set_name}_mcc\": mcc,\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\nMetrics for {set_name} (threshold={threshold}):\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Print confusion matrix values\n",
    "    print(f\"\\nConfusion Matrix Values:\")\n",
    "    print(confusion_str)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_feature_importance(model: Any, feature_cols: list, top_n: int = 20):\n",
    "#     \"\"\"Plot feature importance from XGBoost model.\"\"\"\n",
    "#     importance_scores = model.feature_importances_\n",
    "#     indices = np.argsort(importance_scores)[::-1]\n",
    "\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.title(\"Feature Importances\")\n",
    "#     plt.bar(range(top_n), importance_scores[indices[:top_n]])\n",
    "#     plt.xticks(range(top_n), [feature_cols[i] for i in indices[:top_n]], rotation=90)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Print top N most important features\n",
    "#     print(f\"\\nTop {top_n} most important features:\")\n",
    "#     for i in range(top_n):\n",
    "#         print(f\"{feature_cols[indices[i]]}: {importance_scores[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(\n",
    "    model: Any, scaler: StandardScaler, feature_cols: list, function_name: str\n",
    "):\n",
    "    \"\"\"Save the trained model and scaler.\"\"\"\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "    # Create a dictionary containing all necessary components\n",
    "    model_data = {\"model\": model, \"scaler\": scaler, \"feature_cols\": feature_cols}\n",
    "\n",
    "    # Save the model data\n",
    "    joblib.dump(model_data, f\"../models/{function_name}_predictor.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function_predictor(function_name: str, model_params: Dict[str, Any] = None):\n",
    "    \"\"\"Main training pipeline for a specific protein function.\"\"\"\n",
    "    # # Load cluster mapping\n",
    "    # with open(\"../dataset/protein_cluster_mapping.json\", \"r\") as f:\n",
    "    #     cluster_mapping = json.load(f)\n",
    "\n",
    "    # get df using function name\n",
    "    df = get_df(function_name)\n",
    "    df = subset_negatives(df, ratio=5)\n",
    "\n",
    "    # Cluster positives using diamond results\n",
    "    bitscore_file = \"../dataset/unique_diamond_results.parquet\"\n",
    "    cluster_mapping = cluster_positives(df, bitscore_file, bitscore_cutoff=100)\n",
    "\n",
    "    # Assign dummy group to negatives\n",
    "    for idx, row in df.iterrows():\n",
    "        if row[\"label\"] == 0:\n",
    "            cluster_mapping[row[\"id\"]] = f\"neg_{row['id']}\"\n",
    "\n",
    "    # Get feature columns (excluding 'id' and 'label')\n",
    "    feature_cols = [col for col in df.columns if col not in [\"id\", \"label\"]]\n",
    "\n",
    "    # Prepare data\n",
    "    X, y, ids, groups = prepare_data(df, feature_cols, cluster_mapping)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, ids, groups)\n",
    "\n",
    "    # Train model\n",
    "    model, scaler, evals_result = train_model(\n",
    "        X_train, y_train, X_val, y_val, model_params\n",
    "    )\n",
    "\n",
    "    train_metric = evals_result[\"validation_0\"][\"logloss\"]\n",
    "    val_metric = evals_result[\"validation_1\"][\"logloss\"]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_metric, label=\"Train Logloss\")\n",
    "    plt.plot(val_metric, label=\"Val Logloss\")\n",
    "    plt.xlabel(\"Boosting Round\")\n",
    "    plt.ylabel(\"Logloss\")\n",
    "    plt.title(\n",
    "        f\"Learning Curve: {function_name}\\n\"\n",
    "        f\"n_estimators: {model_params['n_estimators']}, no early stopping\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1.0)\n",
    "    os.makedirs(f\"../results/predictor\", exist_ok=True)\n",
    "    plt.savefig(f\"../results/predictor/{function_name}_learning_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Collect results in a list of strings\n",
    "    results_lines = []\n",
    "\n",
    "    # Feature importances\n",
    "    importance_scores = model.feature_importances_\n",
    "    indices = np.argsort(importance_scores)[::-1]\n",
    "    results_lines.append(\"Top 20 most important features:\")\n",
    "    for i in range(20):\n",
    "        results_lines.append(\n",
    "            f\"{feature_cols[indices[i]]}: {importance_scores[indices[i]]:.4f}\"\n",
    "        )\n",
    "    results_lines.append(\"\")\n",
    "\n",
    "    # Train metrics\n",
    "    train_metrics = evaluate_model(model, scaler, X_train, y_train, \"train\")\n",
    "    results_lines.append(\"=== Training Set ===\")\n",
    "    for k, v in train_metrics.items():\n",
    "        results_lines.append(f\"{k}: {v}\")\n",
    "    results_lines.append(\"\")\n",
    "\n",
    "    # Validation metrics\n",
    "    val_metrics = evaluate_model(model, scaler, X_val, y_val, \"val\")\n",
    "    results_lines.append(\"=== Validation Set ===\")\n",
    "    for k, v in val_metrics.items():\n",
    "        results_lines.append(f\"{k}: {v}\")\n",
    "    results_lines.append(\"\")\n",
    "\n",
    "    # Test metrics for all thresholds\n",
    "    results_lines.append(\"=== Test Set ===\")\n",
    "    thresholds = np.linspace(0.01, 1.0, 100)\n",
    "    best_mcc = -float(\"inf\")\n",
    "    best_threshold = None\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        test_metrics = evaluate_model(model, scaler, X_test, y_test, \"test\", threshold)\n",
    "        results_lines.append(f\"Threshold: {threshold:.2f}\")\n",
    "        for k, v in test_metrics.items():\n",
    "            results_lines.append(f\"{k}: {v}\")\n",
    "        results_lines.append(\"\")\n",
    "        # Track best MCC\n",
    "        mcc = test_metrics.get(\"test_mcc\", -float(\"inf\"))\n",
    "        if isinstance(\n",
    "            mcc, tuple\n",
    "        ):  # If your evaluate_model returns (mcc, confusion_str)\n",
    "            mcc = mcc[0]\n",
    "        if mcc > best_mcc:\n",
    "            best_mcc = mcc\n",
    "            best_threshold = threshold\n",
    "\n",
    "    # Add the best MCC summary line\n",
    "    results_lines.append(f\"Best MCC: {best_mcc:.4f} at threshold {best_threshold:.2f}\")\n",
    "    results_lines.append(\"\")\n",
    "\n",
    "    # Save all results to file\n",
    "    os.makedirs(\"../results/predictor\", exist_ok=True)\n",
    "    result_file = f\"../results/predictor/{function_name}.txt\"\n",
    "    with open(result_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(results_lines))\n",
    "\n",
    "    # Save model\n",
    "    save_model(model, scaler, feature_cols, function_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for testing early stopping\n",
    "# import xgboost\n",
    "\n",
    "# print(\"XGBoost version:\", xgboost.__version__)\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y = load_iris(return_X_y=True)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# model = XGBClassifier(n_estimators=10)\n",
    "# model.fit(\n",
    "#     X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=5, verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params = {\n",
    "#     \"n_estimators\": 10000,\n",
    "#     \"max_depth\": 8,\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"subsample\": 0.7,\n",
    "#     \"colsample_bytree\": 0.7,\n",
    "#     \"min_child_weight\": 2,\n",
    "#     \"random_state\": 42,\n",
    "#     \"early_stopping_rounds\": 100,\n",
    "# }å\n",
    "\n",
    "# train_function_predictor(å\n",
    "#     \"moron_auxiliary_metabolic_gene_and_host_takeover\", model_params\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_names = [\n",
    "    \"lysis\",\n",
    "    \"tail\",\n",
    "    \"connector\",\n",
    "    \"dna_rna_and_nucleotide_metabolism\",\n",
    "    \"head_and_packaging\",\n",
    "    \"other\",\n",
    "    \"transcription_regulation\",\n",
    "    \"moron_auxiliary_metabolic_gene_and_host_takeover\",\n",
    "    \"unknown_function\",\n",
    "    \"integration_and_excision\",\n",
    "]\n",
    "\n",
    "model_params = {\n",
    "    \"n_estimators\": 5000,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"min_child_weight\": 2,\n",
    "    \"random_state\": 42,\n",
    "    # \"early_stopping_rounds\": 100,\n",
    "}\n",
    "\n",
    "for function_name in function_names:\n",
    "    train_function_predictor(function_name, model_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
