{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal: train 10 machine learning predictors: one predictor for each function from the ten protein function categories (\"DNA, RNA and nucleotide metabolism\", \"tail\", \"head and packaging\", \"other\", \"lysis\", \"connector\", \"transcription regulation\", \"moron, auxiliary metabolic gene and host takeover\", \"unknown function\", \"integration and excision\") \n",
    "\n",
    "predictor input: protein features; output: labels (0/1) representing whether the protein serves the specific function\n",
    "\n",
    "dataset: 360,413 seqs in total - 60% for training, 20% for validation, 20% for testing\n",
    "-use clustering results to avoid spliting protein seqs in the same cluster (maybe use GroupShuffleSplit from sklearn)\n",
    "\n",
    "features: 1711-dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset:  \n",
    "2,318,538 seqs in the original dataset  \n",
    "927,040 seqs after dropping pcat \"unknown_no_hit\"  \n",
    "360,413 unique seqs after dropping duplicated seqs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(features, ids):\n",
    "    features[\"label\"] = features[\"id\"].isin(ids[\"name\"]).astype(int)\n",
    "    features = features.drop(columns=[\"md5\"])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_parquet(\"../dataset/protein_features_unique.pa\")\n",
    "lysis_ids = pd.read_csv(\"../dataset/pcat/lysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lysis = add_label(features, lysis_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    339951\n",
       "1     20462\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_lysis[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict, Any\n",
    "import joblib\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: list,\n",
    "    cluster_mapping: Dict[str, str],\n",
    "    label_col: str = \"label\",\n",
    "    id_col: str = \"id\",\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare data for training by separating features and labels.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing features and labels\n",
    "        feature_cols: List of feature column names\n",
    "        cluster_mapping: Dictionary mapping sequence IDs to cluster IDs\n",
    "        label_col: Name of the label column\n",
    "        id_col: Name of the ID column\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix\n",
    "        y: Label array\n",
    "        ids: Array of sequence IDs\n",
    "        groups: Array of cluster IDs for each sequence\n",
    "    \"\"\"\n",
    "    X = df[feature_cols].values\n",
    "    y = df[label_col].values\n",
    "    ids = df[id_col].values\n",
    "\n",
    "    # Get cluster IDs for each sequence\n",
    "    groups = np.array([cluster_mapping.get(str(id_), \"unknown\") for id_ in ids])\n",
    "\n",
    "    return X, y, ids, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    ids: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.2,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Split data into train, validation, and test sets while keeping related sequences together.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Label array\n",
    "        ids: Array of sequence IDs\n",
    "        groups: Array of cluster IDs for each sequence\n",
    "        test_size: Proportion of data to use for testing\n",
    "        val_size: Proportion of data to use for validation\n",
    "    \"\"\"\n",
    "    # First split: separate test set\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "    train_val_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "\n",
    "    # Second split: separate validation set from training set\n",
    "    val_size_adjusted = val_size / (\n",
    "        1 - test_size\n",
    "    )  # Adjust val_size to account for test set\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=val_size_adjusted, random_state=42)\n",
    "    train_idx, val_idx = next(\n",
    "        gss.split(X_train_val, y_train_val, groups=groups_train_val)\n",
    "    )\n",
    "\n",
    "    X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "    y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    X_train: np.ndarray, y_train: np.ndarray, model_params: Dict[str, Any] = None\n",
    ") -> Tuple[Any, StandardScaler]:\n",
    "    \"\"\"Train an XGBoost classifier with optional hyperparameters.\"\"\"\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Set default parameters if none provided\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            \"n_estimators\": 200,\n",
    "            \"max_depth\": 6,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"min_child_weight\": 1,\n",
    "            \"scale_pos_weight\": 1,  # Will be adjusted based on class imbalance\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "    # Calculate scale_pos_weight based on class imbalance\n",
    "    n_neg = np.sum(y_train == 0)\n",
    "    n_pos = np.sum(y_train == 1)\n",
    "    model_params[\"scale_pos_weight\"] = n_neg / n_pos\n",
    "\n",
    "    # Train model\n",
    "    model = XGBClassifier(**model_params)\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train, eval_set=[(X_train_scaled, y_train)], verbose=False\n",
    "    )\n",
    "\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: Any,\n",
    "    scaler: StandardScaler,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    set_name: str = \"\",\n",
    "    threshold: float = 0.5,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate model performance on a dataset.\"\"\"\n",
    "    X_scaled = scaler.transform(X)\n",
    "    y_pred_proba = model.predict_proba(X_scaled)[:, 1]\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    # Calculate confusion matrix values\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate MCC\n",
    "    def matthews(y_true, y_pred):\n",
    "        from math import sqrt\n",
    "\n",
    "        \"\"\"\n",
    "            P  = Total number of positives\n",
    "            N  = Total number of negatives\n",
    "            Tp = number of true positives\n",
    "            Fp = number of false positives\n",
    "        \"\"\"\n",
    "        if type(y_true) == pd.Series:\n",
    "            y_true = y_true.values\n",
    "\n",
    "        P = len([x for x in y_true if x == 1])\n",
    "        N = len([x for x in y_true if x == 0])\n",
    "\n",
    "        Tp, Fp = 0, 0\n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i] == 1 and y_pred[i] == 1:\n",
    "                Tp += 1\n",
    "            elif y_true[i] == 0 and y_pred[i] == 1:\n",
    "                Fp += 1\n",
    "\n",
    "        Tn = N - Fp\n",
    "        Fn = P - Tp\n",
    "\n",
    "        try:\n",
    "            mcc = (Tp * Tn - Fp * Fn) / sqrt(\n",
    "                (Tn + Fn) * (Tn + Fp) * (Tp + Fn) * (Tp + Fp)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            mcc = 0\n",
    "        return mcc\n",
    "\n",
    "    metrics = {\n",
    "        f\"{set_name}_accuracy\": accuracy_score(y, y_pred),\n",
    "        f\"{set_name}_precision\": precision_score(y, y_pred),\n",
    "        f\"{set_name}_recall\": recall_score(y, y_pred),\n",
    "        f\"{set_name}_f1\": f1_score(y, y_pred),\n",
    "        f\"{set_name}_roc_auc\": roc_auc_score(y, y_pred_proba),\n",
    "        f\"{set_name}_mcc\": matthews(y, y_pred),\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\nMetrics for {set_name} (threshold={threshold}):\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Print confusion matrix values\n",
    "    print(f\"\\nConfusion Matrix Values:\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\")\n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model: Any, feature_cols: list, top_n: int = 20):\n",
    "    \"\"\"Plot feature importance from XGBoost model.\"\"\"\n",
    "    importance_scores = model.feature_importances_\n",
    "    indices = np.argsort(importance_scores)[::-1]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.bar(range(top_n), importance_scores[indices[:top_n]])\n",
    "    plt.xticks(range(top_n), [feature_cols[i] for i in indices[:top_n]], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print top N most important features\n",
    "    print(f\"\\nTop {top_n} most important features:\")\n",
    "    for i in range(top_n):\n",
    "        print(f\"{feature_cols[indices[i]]}: {importance_scores[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(\n",
    "    model: Any, scaler: StandardScaler, feature_cols: list, function_name: str\n",
    "):\n",
    "    \"\"\"Save the trained model and scaler.\"\"\"\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "    # Create a dictionary containing all necessary components\n",
    "    model_data = {\"model\": model, \"scaler\": scaler, \"feature_cols\": feature_cols}\n",
    "\n",
    "    # Save the model data\n",
    "    joblib.dump(model_data, f\"../models/{function_name}_predictor.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function_predictor(\n",
    "    df: pd.DataFrame, function_name: str, model_params: Dict[str, Any] = None\n",
    "):\n",
    "    \"\"\"Main training pipeline for a specific protein function.\"\"\"\n",
    "    # Load cluster mapping\n",
    "    with open(\"../dataset/protein_cluster_mapping.json\", \"r\") as f:\n",
    "        cluster_mapping = json.load(f)\n",
    "\n",
    "    # Get feature columns (excluding 'id' and 'label')\n",
    "    feature_cols = [col for col in df.columns if col not in [\"id\", \"label\"]]\n",
    "\n",
    "    # Prepare data\n",
    "    X, y, ids, groups = prepare_data(df, feature_cols, cluster_mapping)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, ids, groups)\n",
    "\n",
    "    # Train model\n",
    "    model, scaler = train_model(X_train, y_train, model_params)\n",
    "\n",
    "    # Print feature importance (without plot)\n",
    "    importance_scores = model.feature_importances_\n",
    "    indices = np.argsort(importance_scores)[::-1]\n",
    "    print(\"\\nTop 20 most important features:\")\n",
    "    for i in range(20):\n",
    "        print(f\"{feature_cols[indices[i]]}: {importance_scores[indices[i]]:.4f}\")\n",
    "\n",
    "    # Evaluate model on all sets\n",
    "    print(\"\\n=== Training Set ===\")\n",
    "    train_metrics = evaluate_model(model, scaler, X_train, y_train, \"train\")\n",
    "\n",
    "    print(\"\\n=== Validation Set ===\")\n",
    "    val_metrics = evaluate_model(model, scaler, X_val, y_val, \"val\")\n",
    "\n",
    "    print(\"\\n=== Test Set ===\")\n",
    "    # Calculate metrics for all thresholds\n",
    "    thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    for threshold in thresholds:\n",
    "        test_metrics = evaluate_model(model, scaler, X_test, y_test, \"test\", threshold)\n",
    "\n",
    "    # Save model\n",
    "    save_model(model, scaler, feature_cols, function_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 most important features:\n",
      "BP:prop_res_Non-polar: 0.0124\n",
      "RED_TRIPEP:HLC: 0.0109\n",
      "BP:isoelectric_point: 0.0104\n",
      "DIPEP:AV: 0.0058\n",
      "DIPEP:VK: 0.0054\n",
      "RED_TRIPEP:PHS: 0.0052\n",
      "RED_TRIPEP:ACS: 0.0052\n",
      "RED_TRIPEP:GLH: 0.0051\n",
      "RED_TRIPEP:PSK: 0.0050\n",
      "BP:prop_res_Polar: 0.0050\n",
      "RED_TRIPEP:PCK: 0.0047\n",
      "BP:molecular_weight: 0.0045\n",
      "RED_TRIPEP:CKF: 0.0044\n",
      "RED_TRIPEP:PPC: 0.0042\n",
      "BP:prop_res_Aliphatic: 0.0041\n",
      "BP:length: 0.0041\n",
      "RED_TRIPEP:ASH: 0.0040\n",
      "RED_TRIPEP:FPL: 0.0038\n",
      "RED_DIPEP:PC: 0.0037\n",
      "RED_DIPEP:FC: 0.0037\n",
      "\n",
      "=== Training Set ===\n",
      "\n",
      "Metrics for train (threshold=0.5):\n",
      "train_accuracy: 0.9984\n",
      "train_precision: 0.9680\n",
      "train_recall: 1.0000\n",
      "train_f1: 0.9838\n",
      "train_roc_auc: 1.0000\n",
      "train_mcc: 0.9830\n",
      "\n",
      "Confusion Matrix Values:\n",
      "True Negatives (TN): 173399\n",
      "False Positives (FP): 296\n",
      "False Negatives (FN): 0\n",
      "True Positives (TP): 8963\n",
      "\n",
      "=== Validation Set ===\n",
      "\n",
      "Metrics for val (threshold=0.5):\n",
      "val_accuracy: 0.9320\n",
      "val_precision: 0.6837\n",
      "val_recall: 0.2083\n",
      "val_f1: 0.3193\n",
      "val_roc_auc: 0.8532\n",
      "val_mcc: 0.3529\n",
      "\n",
      "Confusion Matrix Values:\n",
      "True Negatives (TN): 102877\n",
      "False Positives (FP): 828\n",
      "False Negatives (FN): 6803\n",
      "True Positives (TP): 1790\n",
      "\n",
      "=== Test Set ===\n",
      "\n",
      "Metrics for test (threshold=0.3):\n",
      "test_accuracy: 0.9586\n",
      "test_precision: 0.5321\n",
      "test_recall: 0.5530\n",
      "test_f1: 0.5424\n",
      "test_roc_auc: 0.9073\n",
      "test_mcc: 0.5208\n",
      "\n",
      "Confusion Matrix Values:\n",
      "True Negatives (TN): 61138\n",
      "False Positives (FP): 1413\n",
      "False Negatives (FN): 1299\n",
      "True Positives (TP): 1607\n",
      "\n",
      "Metrics for test (threshold=0.4):\n",
      "test_accuracy: 0.9640\n",
      "test_precision: 0.6122\n",
      "test_recall: 0.5165\n",
      "test_f1: 0.5603\n",
      "test_roc_auc: 0.9073\n",
      "test_mcc: 0.5438\n",
      "\n",
      "Confusion Matrix Values:\n",
      "True Negatives (TN): 61600\n",
      "False Positives (FP): 951\n",
      "False Negatives (FN): 1405\n",
      "True Positives (TP): 1501\n",
      "\n",
      "Metrics for test (threshold=0.5):\n",
      "test_accuracy: 0.9661\n",
      "test_precision: 0.6641\n",
      "test_recall: 0.4776\n",
      "test_f1: 0.5556\n",
      "test_roc_auc: 0.9073\n",
      "test_mcc: 0.5464\n",
      "\n",
      "Confusion Matrix Values:\n",
      "True Negatives (TN): 61849\n",
      "False Positives (FP): 702\n",
      "False Negatives (FN): 1518\n",
      "True Positives (TP): 1388\n",
      "\n",
      "Metrics for test (threshold=0.6):\n",
      "test_accuracy: 0.9666\n",
      "test_precision: 0.6986\n",
      "test_recall: 0.4346\n",
      "test_f1: 0.5359\n",
      "test_roc_auc: 0.9073\n",
      "test_mcc: 0.5353\n",
      "\n",
      "Confusion Matrix Values:\n",
      "True Negatives (TN): 62006\n",
      "False Positives (FP): 545\n",
      "False Negatives (FN): 1643\n",
      "True Positives (TP): 1263\n",
      "\n",
      "Metrics for test (threshold=0.7):\n",
      "test_accuracy: 0.9661\n",
      "test_precision: 0.7203\n",
      "test_recall: 0.3882\n",
      "test_f1: 0.5045\n",
      "test_roc_auc: 0.9073\n",
      "test_mcc: 0.5138\n",
      "\n",
      "Confusion Matrix Values:\n",
      "True Negatives (TN): 62113\n",
      "False Positives (FP): 438\n",
      "False Negatives (FN): 1778\n",
      "True Positives (TP): 1128\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    \"n_estimators\": 300,  # Increase from 200\n",
    "    \"max_depth\": 8,  # Increase from 6\n",
    "    \"learning_rate\": 0.05,  # Decrease from 0.1\n",
    "    \"subsample\": 0.7,  # Decrease from 0.8\n",
    "    \"colsample_bytree\": 0.7,  # Decrease from 0.8\n",
    "    \"min_child_weight\": 2,  # Increase from 1\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# Train the predictor for the \"tail\" function\n",
    "train_function_predictor(features_lysis, \"lysis\", model_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
