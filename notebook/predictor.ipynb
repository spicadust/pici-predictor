{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal: train 10 machine learning predictors: one predictor for each function from the ten protein function categories (\"DNA, RNA and nucleotide metabolism\", \"tail\", \"head and packaging\", \"other\", \"lysis\", \"connector\", \"transcription regulation\", \"moron, auxiliary metabolic gene and host takeover\", \"unknown function\", \"integration and excision\") \n",
    "\n",
    "predictor input: protein features; output: labels (0/1) representing whether the protein serves the specific function\n",
    "\n",
    "dataset: 360,413 seqs in total - 60% for training, 20% for validation, 20% for testing\n",
    "-use clustering results to avoid spliting protein seqs in the same cluster (maybe use GroupShuffleSplit from sklearn)\n",
    "\n",
    "features: 1711-dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset:  \n",
    "2,318,538 seqs in the original dataset  \n",
    "927,040 seqs after dropping pcat \"unknown_no_hit\"  \n",
    "360,413 unique seqs after dropping duplicated seqs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset the negatives;  \n",
    "- for each function, cluster on the positive dataset  \n",
    "- try threshold from 0.01 to 1  \n",
    "- [o] change the dataset splitting strategy  \n",
    "- [o] model param : is_imbalanced  \n",
    "- add learning curve (could be find from XGBoost)  \n",
    "- n_estimators: increased to 10k; may need GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict, Any\n",
    "import joblib\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(function_name: str):\n",
    "    ids = pd.read_csv(f\"../dataset/pcat/{function_name}.csv\")\n",
    "    features = pd.read_parquet(\"../dataset/protein_features_unique.pa\")\n",
    "    features[\"label\"] = features[\"id\"].isin(ids[\"name\"]).astype(int)\n",
    "    features = features.drop(columns=[\"md5\"])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: list,\n",
    "    cluster_mapping: Dict[str, str],\n",
    "    label_col: str = \"label\",\n",
    "    id_col: str = \"id\",\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare data for training by separating features and labels.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing features and labels\n",
    "        feature_cols: List of feature column names\n",
    "        cluster_mapping: Dictionary mapping sequence IDs to cluster IDs\n",
    "        label_col: Name of the label column\n",
    "        id_col: Name of the ID column\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix\n",
    "        y: Label array\n",
    "        ids: Array of sequence IDs\n",
    "        groups: Array of cluster IDs for each sequence\n",
    "    \"\"\"\n",
    "    X = df[feature_cols].values\n",
    "    y = df[label_col].values\n",
    "    ids = df[id_col].values\n",
    "\n",
    "    # Get cluster IDs for each sequence\n",
    "    groups = np.array([cluster_mapping.get(str(id_), \"unknown\") for id_ in ids])\n",
    "\n",
    "    return X, y, ids, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    ids: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.2,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Split data into train, validation, and test sets while keeping related sequences together.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Label array\n",
    "        ids: Array of sequence IDs\n",
    "        groups: Array of cluster IDs for each sequence\n",
    "        test_size: Proportion of data to use for testing\n",
    "        val_size: Proportion of data to use for validation\n",
    "    \"\"\"\n",
    "    # First split: separate test set\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "    train_val_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "\n",
    "    # Second split: separate validation set from training set\n",
    "    val_size_adjusted = val_size / (\n",
    "        1 - test_size\n",
    "    )  # Adjust val_size to account for test set\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=val_size_adjusted, random_state=42)\n",
    "    train_idx, val_idx = next(\n",
    "        gss.split(X_train_val, y_train_val, groups=groups_train_val)\n",
    "    )\n",
    "\n",
    "    X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "    y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# first split: test 20%\n",
    "# train:val = 8:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    model_params: Dict[str, Any] = None,\n",
    ") -> Tuple[Any, StandardScaler]:\n",
    "    \"\"\"Train an XGBoost classifier with optional hyperparameters.\"\"\"\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Set default parameters if none provided\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            \"n_estimators\": 200,\n",
    "            \"max_depth\": 6,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"min_child_weight\": 1,\n",
    "            \"scale_pos_weight\": 1,\n",
    "            \"random_state\": 42,\n",
    "            \"eval_metric\": [\"auc\", \"aucpr\"],  # Move eval_metric here\n",
    "        }\n",
    "\n",
    "    # Calculate scale_pos_weight based on class imbalance\n",
    "    n_neg = np.sum(y_train == 0)\n",
    "    n_pos = np.sum(y_train == 1)\n",
    "    model_params[\"scale_pos_weight\"] = n_neg / n_pos\n",
    "\n",
    "    # Train model\n",
    "    model = XGBClassifier(**model_params)\n",
    "    model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        eval_set=[(X_train_scaled, y_train), (X_val_scaled, y_val)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: Any,\n",
    "    scaler: StandardScaler,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    set_name: str = \"\",\n",
    "    threshold: float = 0.5,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate model performance on a dataset.\"\"\"\n",
    "\n",
    "    # no need for scaling!\n",
    "\n",
    "    X_scaled = scaler.transform(X)\n",
    "    y_pred_proba = model.predict_proba(X_scaled)[:, 1]\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    # Calculate MCC\n",
    "    def matthews(y_true, y_pred):\n",
    "        from math import sqrt\n",
    "\n",
    "        \"\"\"\n",
    "            P  = Total number of positives\n",
    "            N  = Total number of negatives\n",
    "            Tp = number of true positives\n",
    "            Fp = number of false positives\n",
    "        \"\"\"\n",
    "        if type(y_true) == pd.Series:\n",
    "            y_true = y_true.values\n",
    "\n",
    "        P = len([x for x in y_true if x == 1])\n",
    "        N = len([x for x in y_true if x == 0])\n",
    "\n",
    "        Tp, Fp = 0, 0\n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i] == 1 and y_pred[i] == 1:\n",
    "                Tp += 1\n",
    "            elif y_true[i] == 0 and y_pred[i] == 1:\n",
    "                Fp += 1\n",
    "\n",
    "        Tn = N - Fp\n",
    "        Fn = P - Tp\n",
    "\n",
    "        try:\n",
    "            mcc = (Tp * Tn - Fp * Fn) / sqrt(\n",
    "                (Tn + Fn) * (Tn + Fp) * (Tp + Fn) * (Tp + Fp)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            mcc = 0\n",
    "        return (mcc, f\"P: {P:_} Tp: {Tp:_} Fp: {Fp:_} N: {N:_} Tn: {Tn:_} Fn: {Fn:_}\")\n",
    "\n",
    "    # Get MCC and confusion matrix values\n",
    "    mcc, confusion_str = matthews(y, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        f\"{set_name}_accuracy\": accuracy_score(y, y_pred),\n",
    "        f\"{set_name}_precision\": precision_score(y, y_pred),\n",
    "        f\"{set_name}_recall\": recall_score(y, y_pred),\n",
    "        f\"{set_name}_f1\": f1_score(y, y_pred),\n",
    "        f\"{set_name}_roc_auc\": roc_auc_score(y, y_pred_proba),\n",
    "        f\"{set_name}_mcc\": mcc,\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\nMetrics for {set_name} (threshold={threshold}):\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Print confusion matrix values\n",
    "    print(f\"\\nConfusion Matrix Values:\")\n",
    "    print(confusion_str)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_feature_importance(model: Any, feature_cols: list, top_n: int = 20):\n",
    "#     \"\"\"Plot feature importance from XGBoost model.\"\"\"\n",
    "#     importance_scores = model.feature_importances_\n",
    "#     indices = np.argsort(importance_scores)[::-1]\n",
    "\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.title(\"Feature Importances\")\n",
    "#     plt.bar(range(top_n), importance_scores[indices[:top_n]])\n",
    "#     plt.xticks(range(top_n), [feature_cols[i] for i in indices[:top_n]], rotation=90)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Print top N most important features\n",
    "#     print(f\"\\nTop {top_n} most important features:\")\n",
    "#     for i in range(top_n):\n",
    "#         print(f\"{feature_cols[indices[i]]}: {importance_scores[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(\n",
    "    model: Any, scaler: StandardScaler, feature_cols: list, function_name: str\n",
    "):\n",
    "    \"\"\"Save the trained model and scaler.\"\"\"\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "    # Create a dictionary containing all necessary components\n",
    "    model_data = {\"model\": model, \"scaler\": scaler, \"feature_cols\": feature_cols}\n",
    "\n",
    "    # Save the model data\n",
    "    joblib.dump(model_data, f\"../models/{function_name}_predictor.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function_predictor(function_name: str, model_params: Dict[str, Any] = None):\n",
    "    \"\"\"Main training pipeline for a specific protein function.\"\"\"\n",
    "    # Load cluster mapping\n",
    "    with open(\"../dataset/protein_cluster_mapping.json\", \"r\") as f:\n",
    "        cluster_mapping = json.load(f)\n",
    "\n",
    "    # get df using function name\n",
    "    df = get_df(function_name)\n",
    "\n",
    "    # Get feature columns (excluding 'id' and 'label')\n",
    "    feature_cols = [col for col in df.columns if col not in [\"id\", \"label\"]]\n",
    "\n",
    "    # Prepare data\n",
    "    X, y, ids, groups = prepare_data(df, feature_cols, cluster_mapping)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, ids, groups)\n",
    "\n",
    "    # Train model\n",
    "    model, scaler = train_model(X_train, y_train, X_val, y_val, model_params)\n",
    "\n",
    "    # Print feature importance (without plot)\n",
    "    importance_scores = model.feature_importances_\n",
    "    indices = np.argsort(importance_scores)[::-1]\n",
    "    print(\"\\nTop 20 most important features:\")\n",
    "    for i in range(20):\n",
    "        print(f\"{feature_cols[indices[i]]}: {importance_scores[indices[i]]:.4f}\")\n",
    "\n",
    "    # Evaluate model on all sets\n",
    "    print(\"\\n=== Training Set ===\")\n",
    "    train_metrics = evaluate_model(model, scaler, X_train, y_train, \"train\")\n",
    "\n",
    "    print(\"\\n=== Validation Set ===\")\n",
    "    val_metrics = evaluate_model(model, scaler, X_val, y_val, \"val\")\n",
    "\n",
    "    print(\"\\n=== Test Set ===\")\n",
    "    # Calculate metrics for all thresholds\n",
    "    thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]  # try 0.01 - 1\n",
    "    for threshold in thresholds:\n",
    "        test_metrics = evaluate_model(model, scaler, X_test, y_test, \"test\", threshold)\n",
    "\n",
    "    # Save model\n",
    "    save_model(model, scaler, feature_cols, function_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"n_estimators\": 10000,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"min_child_weight\": 2,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "train_function_predictor(\"tail\", model_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
